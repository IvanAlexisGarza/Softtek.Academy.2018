<html>
    <head>
        <link rel="stylesheet" type="text/css" href="Style.css">
        <title>
            Jello
        </title>
    </head>
    <body>
        <main>
            <article>
        <h1 class="center-text">Protecting JPEG Images Against Adversarial Attacks</h1>

        <p class="center-text">Aaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo and James Storer Brandeis University</p>

        <h2 class="center-text">Abstract</h2>
        
        <p class="Justified">As deep neural networks (DNNs) have been integrated into critical systems, several methods to attack these systems have been
        developed. These adversarial attacks make imperceptible modifications to an image that fool DNN classifiers. We present an
        adaptive JPEG encoder which defends against many of these attacks. Experimentally, we show that our method produces images
        with high visual quality while greatly reducing the potency of state-of-the-art attacks. Our algorithm requires only a modest
        increase in encoding time, produces a compressed image which can be decompressed by an off-theshelf JPEG decoder, and classified
        by an unmodified classifier.</p>

        <span>
            <h2>1. Introduction</h2>
            <p class="Justified">Deep neural networks (DNNs) have shown tremendous success in image recognition tasks, even surpassing human capability [8].
            DNNs have become components of many critical systems, such as self-driving cars [1], medical image segmentation, surveillance,
            and malware classification [16]. However, recent research has shown that DNNs are vulnerable to adversarial attacks, in which
            minute, carefully-chosen image perturbations can result in misclassification of the image by the neural network [6]. In most
            cases, this change is imperceptible to humans (the resulting image is visually indistinguishable from the original).
            <br>
            Current
            adversarial attacks take advantage of the over-completeness of the pixel representation of images – that is, storing images
            as an array of values results in storing much more information than is required to recognize the content of an image. Traditional
            image compression techniques, like JPEG, rely on assumptions about the human perception of natural images in order to compress
            them. Since adversarial attacks are imperceptible to the human eye, they can be viewed as hiding themselves in the over-complete
            part of the pixel representation of images. Removing these adversarial perturbations is therefore a lossy image compression
            problem: by effectively encoding natural images, image compression techniques can quantize away imperceptible elements of
            an image, effectively blocking attacks that rely on adversarial perturbations. 
            <br>
            While the research in defending against such
            adversarial perturbations is still in its infancy, it has been shown that JPEG naturally removes some of these imperceptible
            perturbations, thereby restoring the original classification of the image [3, 4]. More advanced attacks, however, are robust
            against JPEG compression. In this work, we study the nature of the perturbations generated by adversarial methods, as well
            as their effect on classification. We present a solution which produces a standard JPEG-decodable image, while allowing significantly
            improved classification recovery as compared to off-the-shelf JPEG compression.
            <br> 
            Several techniques have been proposed which
            attempt to reduce the feasibility of generating adversarial images [15, 12]. Most of these methods involve augmenting the
            DNN training process to create a more robust classifier. However, these defenses often fail against more advanced attacks,
            and come at the cost of more computationally expensive training. 
            <br>
            Standard JPEG compression is able to provide some measure
            of defense against adversarial attacks. Since adversarial perturbations are often high frequency and low magnitude, the quantization
            step of JPEG frequently removes them. When the noise added by the adversary is removed or substantially modified, and the
            resulting image no longer fools the classifier. However, as the severity of quantization increases, important features of
            the original image are also lost, and the model’s ability to correctly classify the resulting image deteriorates regardless
            of any adversarial perturbation. This trade-off limits the effectiveness of a simple JPEG-based defense, as defending against
            more sophisticated attacks requires harsh quantization that renders the output image difficult to classify</p>
        
            <img src="C:\Users\Academia\Documents\Repository\ivan_garza\Modulo 7\Dia uno VSCode\Images\PDF-Intro.PNG" alt="Elefantin">
                </article>
            </main>
        </span>
    </body>
</html>
